{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies: nltk, transformers, np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/UMass/CS685/Project/github_repo/CS685_Sarcasm/'\n",
    "train_df=pd.read_csv(path+\"/train_main_sub.tsv\")\n",
    "test_df=pd.read_csv(path+\"/test_main_sub.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.dropna(subset=[2])\n",
    "test_df=test_df.dropna(subset=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts=train_df['reddit'].values.tolist()\n",
    "test_texts=test_df['reddit'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words=['angry','happy','fear','disgust','sad','surprise']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./w2v/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct =[]\n",
    "punct += list(string.punctuation)\n",
    "punct += 'â€™'\n",
    "punct.remove(\"'\")\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punct:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text=remove_punctuations(text)\n",
    "    toks = nltk.tokenize.word_tokenize(text)\n",
    "#     print(toks)\n",
    "    text = [word for word in toks if word not in stopwords.words('english')]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok='<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens=[]\n",
    "i=0\n",
    "for text in train_texts: \n",
    "    pp_t=text_preprocessing(text)\n",
    "    print(i)\n",
    "    i+=1\n",
    "    if len(pp_t)<max_seq_length:\n",
    "        pad_length=max_seq_length - len(pp_t)\n",
    "#         print(pad_length)\n",
    "        padded_tokens = pp_t + [pad_tok] * pad_length\n",
    "        train_tokens.append(padded_tokens)\n",
    "#         train_tokens.append(pad_tok*pad_length)\n",
    "    elif len(pp_t)>=max_seq_length:\n",
    "        train_tokens.append(pp_t[:max_seq_length])\n",
    "\n",
    "for text in test_texts: \n",
    "    pp_t=text_preprocessing(text)\n",
    "#     print(pp_t)\n",
    "    if len(pp_t)<max_seq_length:\n",
    "        pad_length=max_seq_length - len(pp_t)\n",
    "#         print(pad_length)\n",
    "        padded_tokens = pp_t + [pad_tok] * pad_length\n",
    "        train_tokens.append(padded_tokens)\n",
    "#         train_tokens.append(pad_tok*pad_length)\n",
    "    elif len(pp_t)>=max_seq_length:\n",
    "        train_tokens.append(pp_t[:max_seq_length])\n",
    "    \n",
    "#     stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds=[]\n",
    "for seed in seed_words:\n",
    "    seeds.append(word_vectors[seed])\n",
    "seed_word_vectors=np.array(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine(tokens,seed_word_vectors):\n",
    "    sents=[]\n",
    "    for token in tokens:\n",
    "#         print(token)\n",
    "        try:\n",
    "            wvector=word_vectors.get_vector(token)\n",
    "        except KeyError:\n",
    "            wvector = np.zeros((300,))\n",
    "        \n",
    "        sents.append(wvector)\n",
    "    sent_vector=np.array(sents)\n",
    "    return cosine_similarity(sent_vector,seed_word_vectors)\n",
    "#     x=[]\n",
    "#     for seed in seed_words:\n",
    "#         x.append(model.similarity(word, seed_words))\n",
    "    \n",
    "#     return np.array(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_emo_list=[]\n",
    "for toks in train_tokens:\n",
    "#     print(len(toks))\n",
    "    sim_vecs=get_cosine(toks,seed_word_vectors)\n",
    "#     print((sim_vecs).shape)\n",
    "    sent_emo_list.append(np.mean((sim_vecs),axis=0))\n",
    "sent_emo_vec=np.array(sent_emo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_vecs=np.mean(sent_emo_vec, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('emo_vecs_preprocessed', sent_emo_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
