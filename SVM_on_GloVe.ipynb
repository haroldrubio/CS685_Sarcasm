{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_on_GloVe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpUQz5e9ELeG"
      },
      "source": [
        "# SVM on GloVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3peXSILZTN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebd6985-de46-4fd2-fd10-8458c13aa164"
      },
      "source": [
        "# Build and analyze baseline algorithms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX4FM7qWePad"
      },
      "source": [
        "def process_data(file_name):\n",
        "  x, y = [], []\n",
        "  f = open('/content/drive/MyDrive/data/' + file_name, encoding='utf-8')\n",
        "  for line in f:\n",
        "    _, label, sentence = line.split(\" \", 2)\n",
        "    x.append(sentence.rstrip('\\n'))\n",
        "    y.append(label)\n",
        "  f.close()\n",
        "  x = np.asarray(x)\n",
        "  y = np.asarray(y)\n",
        "  return x, y"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fHVcSG075uB"
      },
      "source": [
        "# load sarcasm data\n",
        "x_tr, y_tr = process_data('train_main.txt')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir4wseW_9zxO"
      },
      "source": [
        "x_te, y_te = process_data('test_main.txt')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95y0M-oe3--",
        "outputId": "115db14f-504d-41d3-fc64-d7177135b039"
      },
      "source": [
        "print(x_tr.shape)\n",
        "print(y_tr.shape)\n",
        "print(x_te.shape)\n",
        "print(y_te.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(257082,)\n",
            "(257082,)\n",
            "(64666,)\n",
            "(64666,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSlP2FhAaryF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a536ce86-829f-4c67-858d-79e58623807f"
      },
      "source": [
        "# step 1: create embeddings dictionary w/ glove.6B.300d.txt\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/My Drive/data/glove.6B.300d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "print('length of word vector: ', len(embeddings_index[\"from\"]))\n",
        "\n",
        "vocab = embeddings_index.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "length of word vector:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrLt4b_5Jqin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8ea144-cabd-4a30-9892-0993c4e46d9c"
      },
      "source": [
        "# 2. for each token in the sentence, get the corresponding word embedding -- something like embedding[token]\n",
        "# tokenize each sentence\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def composite_sentence_embedding(vocab, sent):\n",
        "  arr = np.asarray([embeddings_index[w] for w in sent if w in vocab])\n",
        "  if arr.size == 0: # empty -- none of the words in the sentence exist in the vocab\n",
        "    return np.zeros(shape=(300,))\n",
        "  return np.sum(arr, axis=0)\n",
        "\n",
        "def transform(X, Y):\n",
        "  \"\"\" Transforms sentences into its word embedding form, while eliminating\n",
        "        sentences where all words in the sentence do not exist in the vocabulary\n",
        "    Arguments:\n",
        "        X  (numpy ndarray, shape = (samples,)):\n",
        "            sentences to transform\n",
        "        Y  (numpy ndarray, shape = (samples,)):\n",
        "            sarcasm label 0 or 1\n",
        "\n",
        "    Returns:\n",
        "        x  (numpy ndarray, shape = (N,300)):\n",
        "            remaining sentence embeddings\n",
        "        y  (numpy ndarray, shape = (N,)):\n",
        "            remaining labels\n",
        "        s  (list, shape = (N,)):\n",
        "            remaining sentences\n",
        "  \"\"\"\n",
        "  composite = []\n",
        "  mask = []\n",
        "  sentences = []\n",
        "  for i in range(X.shape[0]):\n",
        "    sentence = X[i]\n",
        "    tokenized = word_tokenize(sentence.lower().strip()) # tokenized sentence\n",
        "    comp = composite_sentence_embedding(vocab, tokenized) # composed sentence embedding\n",
        "    if np.sum(comp) != 0:\n",
        "      composite.append(comp)\n",
        "      mask.append(i)\n",
        "      sentences.append(sentence)\n",
        "  composite = np.stack(composite)\n",
        "  return composite, Y[mask], sentences"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytSZ0IFG6TV"
      },
      "source": [
        "X_tr, Y_tr, _ = transform(x_tr, y_tr) "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SBh1eAVGxQr"
      },
      "source": [
        "X_te, Y_te, te_sent = transform(x_te, y_te)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM_OLzljSTX0",
        "outputId": "82793d68-230b-4a8c-dfda-26a961aed860"
      },
      "source": [
        "# 80-10-10 split of train/validation/test  \n",
        "\n",
        "# split test into validation and test sets -- should only run this cell once\n",
        "n = round(X_te.shape[0]/2)\n",
        "X_val, Y_val, val_sent = X_te[:n], Y_te[:n], te_sent[:n]\n",
        "X_te, Y_te, te_sent = X_te[n:], Y_te[n:], te_sent[n:]\n",
        "print(X_te.shape, Y_te.shape, len(te_sent))\n",
        "print(X_val.shape, Y_val.shape, len(val_sent))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32112, 300) (32112,) 32112\n",
            "(32112, 300) (32112,) 32112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSkRYfktePB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af38a0e-95b0-49c0-f284-ff484aecd4f8"
      },
      "source": [
        "# 3. fit the linear svc on training data, where x contains word embeddings for each sentence, and y is 0/1 for sarcasm\n",
        "# also hyperparameter tuning\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# roughly 2 minutes to train\n",
        "\n",
        "svc = LinearSVC(C=0.05, dual=False) # dual=False when n_samples > n_features\n",
        "svc.fit(X_tr, Y_tr)\n",
        "print(svc.score(X_val,Y_val)) # 0.6150660189337319"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6150660189337319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU5M_HU0Kfz1",
        "outputId": "ae42c018-b92c-468a-c0aa-259beb592e1d"
      },
      "source": [
        "# retrain model on X_tr + X_val\n",
        "X_comb = np.concatenate((X_tr, X_val))\n",
        "Y_comb = np.concatenate((Y_tr, Y_val))\n",
        "svc.fit(X_comb,Y_comb)\n",
        "\n",
        "# get accuracy on test set\n",
        "print(svc.score(X_te,Y_te)) # 0.6153774289985052"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6153774289985052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hCbNATXlzfC",
        "outputId": "401720fc-5534-4917-bbb6-b197ecdaab34"
      },
      "source": [
        "# need to include precision, recall, F1 scores\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "y_pred = svc.predict(X_te)\n",
        "print('precision score: ', precision_score(Y_te, y_pred, pos_label='1'))\n",
        "print('recall score: ', recall_score(Y_te, y_pred, pos_label='1'))\n",
        "print('f1 score: ', f1_score(Y_te, y_pred, pos_label='1')) # score for positive label\n",
        "print()\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision_recall_fscore_support(Y_te, y_pred, labels=['0','1']) # scores for neg/pos label\n",
        "# number of negative labels: 16005\n",
        "# number of positive labels: 16107"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision score:  0.6231152484594205\n",
            "recall score:  0.5901160985906748\n",
            "f1 score:  0.6061668951882911\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.60837585, 0.62311525]),\n",
              " array([0.64079975, 0.5901161 ]),\n",
              " array([0.624167 , 0.6061669]),\n",
              " array([16005, 16107]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqYEymK8R053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aa1563-15fd-459a-94a7-9ac6ddac91e3"
      },
      "source": [
        "# error analysis -- what examples does the model fail at? take 100-200 validation \n",
        "# set examples that your model incorrectly predicted and categorize them\n",
        "# model cannot predict sentences where word embeddings don't exist\n",
        "\n",
        "predictions = svc.predict(X_val)\n",
        "mask = Y_val != predictions\n",
        "misclassified = np.asarray(val_sent)[mask][:100]\n",
        "print(misclassified)\n",
        "\n",
        "# np.save('/content/drive/MyDrive/data/misclassified_sentences.npy', misclassified)\n",
        "# d = np.load('/content/drive/MyDrive/data/misclassified_sentences.npy')\n",
        "# print(d)\n",
        "\n",
        "# it's a useful debugging tool -- predict 0; could be interpreted as non-sarcastic, really depends on more context\n",
        "# public interface someservice { void provideservice ( ) throws checkedexceptionofeverypossibleimplementationtype } neat abstraction -- makes no sense\n",
        "# 'hmm ... cdj-2000 for $ 900 ... seems legit -- probably hmm was not in vocab -- couldn't understand shift in tone\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['chalk it up to the ever-increasing cost of freedom .'\n",
            " \"we 're about to finally get affordable housing , and now the politicians are doing everything they can to keep prices * high * .\"\n",
            " \"it 's a useful debugging tool\"\n",
            " 'including my peers in 5th grade , who did their series report on this topic !'\n",
            " 'well that makes me want to vote for them .'\n",
            " \"why is it that the minority parties seem incapable of producing any publicity material that is n't , y'know , shit ?\"\n",
            " \"this 'll end well .\"\n",
            " \"i only read the headline but , i do n't think a survey is too accurate unless you talk to more than 30 travelers .\"\n",
            " 'should be labeled nsfw , and might be illegal in some countries with some child nudity there .'\n",
            " 'public interface someservice { void provideservice ( ) throws checkedexceptionofeverypossibleimplementationtype } neat abstraction'\n",
            " 'clearly engadget is using the coercive power of the state to attack their rivals over at gizmodo .'\n",
            " \"congratulations- you 'll be homeless before you get a job .\"\n",
            " \"i guess no one at google 's ever been on a plane and wanted to listen to their music library .\"\n",
            " 'this is really surprising .'\n",
            " \"us political pressure might not mean much with countries like india and china , multi-polar world ca n't come fast enough .\"\n",
            " 'i reacted in a similar manner when i found out about the remake of the hobbit .'\n",
            " \"i made it as far as `` hi ! ''\" 'welcome to china' 'cpr fail .'\n",
            " \"you ca n't , like , own land , maaan .\" 'fucking commies'\n",
            " \"i 'm soooo sad .\"\n",
            " \"thank god ... i wouldv ' e hated for them to be rewarded for shenanigans .\"\n",
            " 'hmm ... cdj-2000 for $ 900 ... seems legit'\n",
            " \"clearly we 're so hot right now\"\n",
            " 'i mean , why interview actual people when you can pull a ( n unverified ) blurb off of the internet ?'\n",
            " \"* '' sup brah '' *\"\n",
            " 'obligatory post about tyler seguin only being 19 and the world should be outraged .'\n",
            " 'drawing attention to this long proven statistic is racist .'\n",
            " 'be careful in brooklyn' 'in other news obama pisses in ocean'\n",
            " 'yikes , no range at all on that team .'\n",
            " 'the otherside of the coin is that they just moved up to a higher tax bracket .'\n",
            " 'drat , there goes our libertarian paradise .'\n",
            " 'is it me , or are there a lot of photos showing protestors attacking police .... bet there are photos showing police attacking protestors ... i wonder why they are missing ?'\n",
            " 'heartwarming !' \"pretty sure those people ca n't afford the internet .\"\n",
            " \"i wonder why it is n't thought men should have access to free contraception .\"\n",
            " 'in the next 10 chapters , kakashi reveals he has the flying thunder god technique and finishes tobi off ... that showing it that the main character does not have to do anything .'\n",
            " 'spoiler alert : tobi is obito'\n",
            " 'its reasons like this that make me hate the fact that im a pakistani !'\n",
            " \"well , yeah , but his plan would funnel money to job creators so they can store it safely until they 're ready to make a job for one of those guys maybe .\"\n",
            " 'you know what they say , the more the merrier .'\n",
            " 'you need the ivory dragon claw'\n",
            " \"dont link to your playlist just so your other video 's get more views , it 's annoying and not userfriendly .\"\n",
            " 'yes , the fall if romney wins' 'this is assuming america rose at all .'\n",
            " \"file under `` how the right wing wants to keep government smaller and out of your business . ''\"\n",
            " 'i wonder who will win ?' 'billions of dollars well spent .'\n",
            " 'please tell me that multi billion dollar rover has the ability to take pictures in color .'\n",
            " 'this never gets old'\n",
            " 'i never understood why on earth we can make crystal clear high res pictures , but all the pictures from rovers and probes and what not are pretty unclear can someone please explain ?'\n",
            " 'arsenal downvote brigade assemble !'\n",
            " \"`` when will this anti-christian bigotry end ? ? ? ? ''\"\n",
            " \"im sure alex will be gnashing his teeth and ranting about how this confirms everything he 's been gnashing his teeth and ranting about .\"\n",
            " \"that does n't look photoshopped at all .\"\n",
            " 'very impressive bombing run , well played lads !'\n",
            " \"you sure that was n't `` boy '' ?\" 'how so'\n",
            " 'anyone who listened to love line would know that .'\n",
            " '... but he seems like such a nice guy .'\n",
            " \"i like romney 's advice better - `` borrow from your folks '' .\"\n",
            " \"* back * into chains does n't refer to blacks at all and * that * does n't refer to a business ... i guess this is proving that public edumacation is such a great program\"\n",
            " 'no i think that was a pretend map they made up for the press conference .'\n",
            " 'best thread in months !'\n",
            " 'that poll is flawed as i can only choose one class for main , i should be able to choose all of the above'\n",
            " \"so basically ... it 's sorta like evony and world of fight ?\"\n",
            " 'sounds like an amazing elected official .'\n",
            " 'look at the source , this is obviously a liberal conspiracy theory .'\n",
            " \"fortunately , evolution will likely solve this issue even if we do n't .\"\n",
            " 'did he live ?' 'lucky russians' 'whats a blackberry ?'\n",
            " 'shut up and take my money'\n",
            " 'i enjoy seeing this picture every week .. you know .' 'what oil spill ?'\n",
            " 'yeah , but how do they taste ?'\n",
            " \"i hate it when cats do this ... ... i 'm always talking to my cat about this `` tibbles '' , i say `` this really cant go on ''\"\n",
            " 'this subreddit definitely needs more circlejerk of jeff and dean .'\n",
            " \"he forgot to say `` no homo '' .\"\n",
            " 'good , the government will run out of money sooner and hopefully collapse .'\n",
            " 'i vote for open enrollment .... i have a right to food ..... lets given every person ( not citizen , every person ) and $ 100 credit at the grocery store of their choice every week ... only then will the evil rich pay their fair share'\n",
            " 'thank you for reminding me of this horrible day in east coast us history .'\n",
            " 'glad to see you guys go horde .' 'nice cheek weld .'\n",
            " 'like the video ... i love g3 , awsome gun !'\n",
            " 'the parties always follow their platforms .'\n",
            " 'because before apple there was no such thing as a store .'\n",
            " 'cleary , from these three pictures , we can tell this mother is a horrible mother and it is her fault her daughter got pregnant .'\n",
            " 'surely , you are the voice of jesus himself .'\n",
            " 'i think this further enforces the fact that you should have to enter a drivers license number to have a facebook account .'\n",
            " 'away with your youtube style comments !'\n",
            " \"i 've only got 64bit java installed and use a 32 bit browser ... should be okay , right ?\"\n",
            " 'but what if i need it ?' 'wow , that is awesome !'\n",
            " 'fucking backwards southerners'\n",
            " 'hate crimes happened everywhere , not just the jim crow south .'\n",
            " 'i hope they bring that styling to their next generation of phones .'\n",
            " \"ca n't wait to take high quality 16mp photos and filter the quality out of it with instagram .\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llNUcotIpx1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c7cc2a-2208-43ca-8832-446a883f1b70"
      },
      "source": [
        "# ensure that there is no data w/ all zeros\n",
        "print(np.all(np.sum(X_tr, axis=1)))\n",
        "print(np.all(np.sum(X_te, axis=1)))\n",
        "print(np.all(np.sum(X_val, axis=1)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zFyfyE2I6wG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}